{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HWK 3 -- part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Make a linear module (call it mod) with the following weight matrix and bias vector:\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "1 &1 \\\\ 2 & 0 \\\\ -1 & 1\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "b = \\begin{bmatrix}\n",
    "1  \\\\ 1 \\\\ -1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "(Hint: the weights and bias are going to be initialized randomly, then you will need to change them one by one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.,  1.],\n",
      "        [ 2.,  0.],\n",
      "        [-1.,  1.]], grad_fn=<CopySlices>)\n",
      "Parameter containing:\n",
      "tensor([ 1.,  1., -1.], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "mod = nn.Linear(2,3,bias=True)\n",
    "W = [[1,1], [2,0], [-1,1]]\n",
    "b = [1,1,-1]\n",
    "for i in range(len(W)):\n",
    "    for j in range(len(W[0])):\n",
    "        mod.weight[i][j] = W[i][j]\n",
    "for i in range(len(b)) : mod.bias[i] = b[i] \n",
    "print(mod.weight)\n",
    "print(mod.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Make a vector $x = \\begin{bmatrix} 2  \\\\ 1  \\end{bmatrix} $  and feed it to the module. Print the output. \n",
    "(Hint: make sure to use torch.Tensor with a capital T! If you use torch.tensor you will get a bug. Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  5., -2.], grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([2,1])\n",
    "y = mod(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Check with pen and paper that pytorch got the correct answer. (I will collect it, so write it clearly.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Make a 3 layer neural net as follow:\n",
    "$$\n",
    "\\text{Linear layer 1} \\to  \\text{ReLU} \\to \\text{Linear layer 2} \\to  \\text{ReLU} \\to \\text{Linear layer 3}\n",
    "$$\n",
    "### Set the weight matrices of the three linear layers to be \n",
    "$$\n",
    "W_1 \n",
    "= \\begin{bmatrix}\n",
    "1 &1 \\\\ 0 & -1 \n",
    "\\end{bmatrix} \\qquad \\text{,} \\qquad\n",
    "W_2 \n",
    "= \\begin{bmatrix}\n",
    "-1 &1 \\\\ 1 & -1 \n",
    "\\end{bmatrix}\n",
    "\\qquad \\text{and} \\qquad\n",
    "W_3 \n",
    "= \\begin{bmatrix}\n",
    "0 &2 \\\\ 1 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "### Let's not put any bias. Put a ReLU nonlinearity between the linear layers, but do NOT put any softmax at the end (so the network will output raw scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class three_layer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1 , bias=False)\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2 , bias=False)\n",
    "        self.layer3 = nn.Linear(hidden_size2, output_size, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)     \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Make a vector $x = \\begin{bmatrix} 2  \\\\ 1  \\end{bmatrix} $  and feed it to the network. Print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 3.], grad_fn=<ReluBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = three_layer_net(2,2,2,2)\n",
    "\n",
    "net.layer1.weight[0][0] = 1\n",
    "net.layer1.weight[0][1] = 1\n",
    "net.layer1.weight[1][0] = 0\n",
    "net.layer1.weight[1][1] = -1\n",
    "        \n",
    "net.layer2.weight[0][0] = -1\n",
    "net.layer2.weight[0][1] = 1\n",
    "net.layer2.weight[1][0] = 1\n",
    "net.layer2.weight[1][1] = -1   \n",
    "        \n",
    "net.layer3.weight[0][0] = 0\n",
    "net.layer3.weight[0][1] = 2\n",
    "net.layer3.weight[1][0] = 1\n",
    "net.layer3.weight[1][1] = 1\n",
    "\n",
    "x = torch.Tensor([2,1])\n",
    "y = net(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Check with pen and paper that pytorch got the correct answer. (I will collect it, so write it carefully)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Use Pytorch to compute the softmax of the following vectors:\n",
    "$$\n",
    "x = \\begin{bmatrix} 1  \\\\ 2 \\\\-2  \\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2654, 0.7214, 0.0132])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([1,2,-2])\n",
    "p = F.softmax(x, dim=0)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Check with simple python command that pytorch correctly computed the softmax. Follow the steps below (note that you need to import the package \"math\" in order to have access to the exponential function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459045\n",
      "7.38905609893065\n",
      "0.1353352832366127\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# compute the exponential of the thre entries of the vector:\n",
    "y1 = math.exp(1)\n",
    "y2 = math.exp(2)\n",
    "y3 = math.exp(-2)\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26538792877224193\n",
      "0.7213991842739688\n",
      "0.013212886953789417\n"
     ]
    }
   ],
   "source": [
    "# then divide by the sum and you should recover the values computed by pytorch\n",
    "sum_y = y1+y2+y3\n",
    "p1= y1/sum_y\n",
    "p2= y2/sum_y\n",
    "p3= y3/sum_y\n",
    "print(p1)\n",
    "print(p2)\n",
    "print(p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Create the matrix\n",
    "$$\n",
    "x = \\begin{bmatrix} 1  & 2 & -2  \\\\ 1  & 5 & -2  \\end{bmatrix} \n",
    "$$\n",
    "### Note that the first row is the vector from part a). Compute the softmax of this matrix in the dimension 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2654, 0.7214, 0.0132],\n",
      "        [0.0180, 0.9811, 0.0009]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1,2,-2], [1,5,-2]])\n",
    "p = F.softmax(x, dim=1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Explain in the cell below why the result you got make sense. In order to write in a cell (rather than puting some code, you need to switch from \"code\" to \"Markdown\" in the box right below the Widget and Help)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we put dim 1 that means to compute the softmax over the first dimension which are the columns, that means that I want all the colums values to add up to one (for every row). Hence the softmax is produced on tensors [1,2,-2] and [1,5,-2]. The values make sense, for the first tensor, 2 is the biggest value hence it will be bigger after softmax. Similarly for second tensor, we can see that the only difference from the other tensor is the middle value which is changed from 2 -> 5 this makes a huge difference on te softmax result, again since we are exponentiating, the 5 becomes very large taking a value of 0.9811. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now compute the softmax in the dimension 0, and explain why it makes sense (in particular, explain the 50% probabilities in the first and last columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.0474, 0.5000],\n",
      "        [0.5000, 0.9526, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1,2,-2], [1,5,-2]])\n",
    "p = F.softmax(x, dim=0)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are computing the softmax over the dimension 0 which means that for every column we add up the value on each row and we want it to add to one and performe the softmax on those. This means the softmax is applied on the following [1,1], [2,5], [-2,-2]. The first column has two values 1 and 1, the softmax will give 50% on both because these values are identical ( e^1/2e^1 = 1/2) same argument goes for the last column. for the second column, 5 is greater than 2 hence we see a bigger value of 0.9526 allocated to that position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
