{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUIZ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an input image with 2 channels:\n",
    " $$\n",
    " \\text{channel 1} = \n",
    " \\begin{bmatrix}\n",
    " 3&2&1\\\\\n",
    " 2&1&2\\\\\n",
    " 1&2&3  \n",
    " \\end{bmatrix}\n",
    " \\qquad  \\text{channel 2} = \n",
    " \\begin{bmatrix}\n",
    " 1&2&3\\\\\n",
    " 2&1&2\\\\\n",
    " 3&2&1\n",
    " \\end{bmatrix}\n",
    " $$\n",
    " and a convolutional module with one filter only:\n",
    " \n",
    " $$\n",
    " \\text{channel 1} = \n",
    " \\begin{bmatrix}\n",
    " 1&1 \\\\\n",
    " 1&1\n",
    " \\end{bmatrix}\n",
    " \\qquad \n",
    " \\text{ channel 2} = \n",
    " \\begin{bmatrix}\n",
    " 1&0 \\\\\n",
    " 1&1\n",
    " \\end{bmatrix}\n",
    " $$\n",
    "Your goal is to compute the resulting activation map. You are strongly encouraged to check your result with pen and paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the convolutional module (set bias to False, don't use any padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = nn.Conv2d(2,1, kernel_size=2, padding=0, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the internal parameters of the convolutional module so that they are equal to the  desired filter. Be mindfull of the dimention. Likely you are going to use the functions \n",
    "* torch.stack()\n",
    "* view()\n",
    "* nn.Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 2, 2])\n",
      "Parameter containing:\n",
      "tensor([[[[1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 0.],\n",
      "          [1., 1.]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# filt_ch1 = torch.Tensor([[1,1],[1,1]])\n",
    "# filt_ch2 = torch.Tensor([[1,0],[1,1]])\n",
    "# filter_ = nn.Parameter(torch.stack((filt_ch1,filt_ch2), dim=0))\n",
    "\n",
    "filter_  = nn.Parameter(torch.Tensor([[[[1,1],[1,1]],[[1,0],[1,1]]]]))\n",
    "print(filter_.size())\n",
    "mod.weight = filter_\n",
    "print(mod.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the input picture:\n",
    "$$\n",
    " \\text{channel 1} = \n",
    " \\begin{bmatrix}\n",
    " 3&2&1\\\\\n",
    " 2&1&2\\\\\n",
    " 1&2&3  \n",
    " \\end{bmatrix}\n",
    " \\qquad  \\text{channel 2} = \n",
    " \\begin{bmatrix}\n",
    " 1&2&3\\\\\n",
    " 2&1&2\\\\\n",
    " 3&2&1\n",
    " \\end{bmatrix}\n",
    " $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3., 2., 1.],\n",
      "         [2., 1., 2.],\n",
      "         [1., 2., 3.]],\n",
      "\n",
      "        [[1., 2., 3.],\n",
      "         [2., 1., 2.],\n",
      "         [3., 2., 1.]]])\n",
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "ch1 = torch.Tensor([[3,2,1], [2,1,2], [1,2,3]])\n",
    "ch2 = torch.Tensor([[1,2,3], [2,1,2], [3,2,1]])\n",
    "img = torch.stack((ch1,ch2), dim=0)\n",
    "print(img)\n",
    "print(img.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the convolution and print the resulting activation map. Don't forget that you need to take care of the batch size: Use the command view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "bs=1\n",
    "img = img.view(bs,2,3,3)\n",
    "print(img.size())\n",
    "act_map = mod(img)\n",
    "#act_map = mod(img.unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[12., 11.],\n",
      "          [13., 12.]]]], grad_fn=<ThnnConv2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(act_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
